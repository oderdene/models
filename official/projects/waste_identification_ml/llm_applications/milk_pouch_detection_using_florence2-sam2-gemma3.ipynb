{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73Uw4PfHv6sv",
      "metadata": {
        "id": "73Uw4PfHv6sv"
      },
      "source": [
        "# **Automatic Mask Generation Using Unsupervised Approach with Florence-2, SAM2, and Gemma3**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kMY1UaQMwX3O",
      "metadata": {
        "id": "kMY1UaQMwX3O"
      },
      "source": [
        "In this notebook, we build an end-to-end unsupervised pipeline for object detection, segmentation, classification, and tracking—focusing on identifying and following milk pouches without manual labels. This approach leverages cutting-edge vision and language models and concludes with lightweight object tracking based on extracted features from segmentation masks.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Florence-2 Multimodal Model**\u003cbr\u003e\n",
        "A powerful vision-language model that performs generic object detection by returning bounding boxes around visually significant regions—completely label-free and prompt-driven.\n",
        "2.   **SAM2 (Segment Anything Model v2)**\u003cbr\u003e\n",
        "Using the bounding boxes from Florence-2, SAM2 generates precise segmentation masks, enabling instance-level understanding and clean extraction of objects.\n",
        "3.  **Gemma3 12B QAT Model**\u003cbr\u003e\n",
        "Each cropped masked region is passed to an open source Gemma3 quantization-aware large language model to determine whether it contains a milk pouch or not, enabling robust classification without explicit supervised training.\n",
        "4.  **Object Tracking via Mask Features**\u003cbr\u003e\n",
        "For the final step, we extract distinguishing features from the segmented masks of positively identified milk pouches and use them to track the same objects across frames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8w6XSOwm7rto",
      "metadata": {
        "id": "8w6XSOwm7rto"
      },
      "source": [
        "While this colab focuses on the specific requirement of distinguishing milk sachets from other types (such as oil), the general approach could easily be adapted for other objects or use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4h669bAtw-XX",
      "metadata": {
        "id": "4h669bAtw-XX"
      },
      "source": [
        "## Install and upgrade the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MrAxLioBwfWg",
      "metadata": {
        "id": "MrAxLioBwfWg"
      },
      "outputs": [],
      "source": [
        "# Install the SAM2 (Segment Anything Model v2) library directly from the official Facebook Research GitHub repository\n",
        "!pip install 'git+https://github.com/facebookresearch/sam2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pJgWgODlibbB",
      "metadata": {
        "id": "pJgWgODlibbB"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y pciutils lshw\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p4FMrtmls2KM",
      "metadata": {
        "id": "p4FMrtmls2KM"
      },
      "outputs": [],
      "source": [
        "# download the sample image from the circularnet project\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/tensorflow/models/master/official/\"\n",
        "    \"projects/waste_identification_ml/pre_processing/config/sample_images/\"\n",
        "    \"IMG_6509.png\"\n",
        ")\n",
        "\n",
        "!curl -O {url} \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzOqkxvkxUoy",
      "metadata": {
        "id": "FzOqkxvkxUoy"
      },
      "source": [
        "## Import the libraries and configure resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SA-Qv2UkxYWU",
      "metadata": {
        "id": "SA-Qv2UkxYWU"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import sys\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import math\n",
        "import cv2\n",
        "import tempfile\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ollama import chat\n",
        "from ollama import ChatResponse\n",
        "import gc\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "\n",
        "# select the device for computation\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    # use bfloat16 for the entire notebook\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
        "    if torch.cuda.get_device_properties(0).major \u003e= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YCcsbDHdlk8G",
      "metadata": {
        "id": "YCcsbDHdlk8G"
      },
      "outputs": [],
      "source": [
        "#@title Utils\n",
        "\n",
        "\n",
        "def free_gpu_vars(*var_names, scope=None):\n",
        "  \"\"\"\n",
        "  Deletes variables (by name) from the provided scope (globals or locals),\n",
        "  collects garbage, and empties the CUDA cache.\n",
        "  \"\"\"\n",
        "  if scope is None:\n",
        "    scope = globals()\n",
        "  for var in var_names:\n",
        "    if var in scope:\n",
        "      del scope[var]\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def plot_bbox(image, data):\n",
        "  # Create a figure and axes\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Display the image\n",
        "  ax.imshow(image)\n",
        "\n",
        "  # Plot each bounding box\n",
        "  for bbox, label in zip(data['bboxes'], data['labels']):\n",
        "    # Unpack the bounding box coordinates\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    # Create a Rectangle patch\n",
        "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "    # Add the rectangle to the Axes\n",
        "    ax.add_patch(rect)\n",
        "    # Annotate the label\n",
        "    plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "  # Remove the axis ticks and labels\n",
        "  ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def run_example(task_prompt, text_input=None):\n",
        "  if text_input is None:\n",
        "    prompt = task_prompt\n",
        "  else:\n",
        "    prompt = task_prompt + text_input\n",
        "  inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
        "  generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].cuda(),\n",
        "    pixel_values=inputs[\"pixel_values\"].cuda(),\n",
        "    max_new_tokens=1024,\n",
        "    early_stopping=False,\n",
        "    do_sample=False,\n",
        "    num_beams=3,\n",
        "  )\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "  parsed_answer = processor.post_process_generation(\n",
        "    generated_text,\n",
        "    task=task_prompt,\n",
        "    image_size=(image.width, image.height)\n",
        "  )\n",
        "\n",
        "  return parsed_answer\n",
        "\n",
        "\n",
        "def show_mask(\n",
        "        mask,\n",
        "        ax,\n",
        "        random_color=False,\n",
        "        borders = True\n",
        "):\n",
        "  if random_color:\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "  else:\n",
        "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "  h, w = mask.shape[-2:]\n",
        "  mask = mask.astype(np.uint8)\n",
        "  mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "  if borders:\n",
        "    import cv2\n",
        "    contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    # Try to smooth contours\n",
        "    contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "    mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "  ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(\n",
        "        coords,\n",
        "        labels,\n",
        "        ax,\n",
        "        marker_size=375\n",
        "):\n",
        "  pos_points = coords[labels==1]\n",
        "  neg_points = coords[labels==0]\n",
        "  ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "  ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "  x0, y0 = box[0], box[1]\n",
        "  w, h = box[2] - box[0], box[3] - box[1]\n",
        "  ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
        "\n",
        "\n",
        "def show_masks(\n",
        "        image,\n",
        "        masks,\n",
        "        scores,\n",
        "        point_coords=None,\n",
        "        box_coords=None,\n",
        "        input_labels=None,\n",
        "        borders=True\n",
        "):\n",
        "  for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    show_mask(mask, plt.gca(), borders=borders)\n",
        "    if point_coords is not None:\n",
        "        assert input_labels is not None\n",
        "        show_points(point_coords, input_labels, plt.gca())\n",
        "    if box_coords is not None:\n",
        "        # boxes\n",
        "        show_box(box_coords, plt.gca())\n",
        "    if len(scores) \u003e 1:\n",
        "        plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3w2HICPxpIu",
      "metadata": {
        "id": "f3w2HICPxpIu"
      },
      "source": [
        "## Read an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9A3H9atRxqbk",
      "metadata": {
        "id": "9A3H9atRxqbk"
      },
      "outputs": [],
      "source": [
        "path = 'IMG_6509.png'\n",
        "image = Image.open(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bGPfN2m-x-7T",
      "metadata": {
        "id": "bGPfN2m-x-7T"
      },
      "source": [
        "## Download Florence-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xeoo3uRmyERg",
      "metadata": {
        "id": "xeoo3uRmyERg"
      },
      "outputs": [],
      "source": [
        "model_id = 'microsoft/Florence-2-large'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Qvac4GAxtDE",
      "metadata": {
        "id": "8Qvac4GAxtDE"
      },
      "outputs": [],
      "source": [
        "# Perform object detection using Florence-2 OD task to detect all bboxes.\n",
        "task_prompt = '\u003cCAPTION_TO_PHRASE_GROUNDING\u003e'\n",
        "results = run_example(task_prompt, text_input=\"packets.\")\n",
        "plot_bbox(image, results['\u003cCAPTION_TO_PHRASE_GROUNDING\u003e'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zxnfK3UExuzb",
      "metadata": {
        "id": "zxnfK3UExuzb"
      },
      "outputs": [],
      "source": [
        "free_gpu_vars('model', 'processor')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KoxQ7DynyQcN",
      "metadata": {
        "id": "KoxQ7DynyQcN"
      },
      "source": [
        "## Download SAM-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O57RyfEayo-_",
      "metadata": {
        "id": "O57RyfEayo-_"
      },
      "outputs": [],
      "source": [
        "# Create the 'checkpoints' directory one level up if it doesn't already exist\n",
        "!mkdir -p checkpoints/\n",
        "\n",
        "# Download the pre-trained SAM2.1 Hiera Large model checkpoint into the 'checkpoints' directory\n",
        "!wget -P checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "\n",
        "# Path to the pre-trained SAM2 model checkpoint\n",
        "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
        "\n",
        "# Path to the configuration file for the SAM2 model variant being used\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "# Build the SAM2 model using the config and checkpoint; `device` should be set to \"cuda\" or \"cpu\"\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "# Create a predictor object using the loaded SAM2 model for image-based mask prediction\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "# Perform segmentation on bbox cordinates using SAM2 model.\n",
        "sam2_predictor.set_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3QUFPffLyKaN",
      "metadata": {
        "id": "3QUFPffLyKaN"
      },
      "outputs": [],
      "source": [
        "# Create a directory to store the cropped object images.\n",
        "os.makedirs('tempdir', exist_ok=True)\n",
        "\n",
        "# Use bounding boxes to extract mask for each object..\n",
        "for idx, bbox in tqdm.tqdm(enumerate(results['\u003cCAPTION_TO_PHRASE_GROUNDING\u003e']['bboxes'])):\n",
        "  x1, y1, x2, y2 = list(map(round, bbox))\n",
        "  if (x2-x1)*(y2-y1) \u003c 0.25 * math.prod(image.size):\n",
        "    input_box = np.array([x1, y1, x2, y2])\n",
        "\n",
        "    masks, scores, _ = sam2_predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    # show_masks(image, masks, scores, box_coords=input_box)\n",
        "\n",
        "    # Convert the first mask to 0-255 and expand its dimensions to match the image channels.\n",
        "    # Multiply the mask with the original image (preserves object, sets background to 0).\n",
        "    # Crop the masked image to the bounding box [y1:y2, x1:x2].\n",
        "    masked_object = Image.fromarray(\n",
        "        np.where(\n",
        "            np.expand_dims(masks[0]*255, -1),\n",
        "            np.array(image), 0\n",
        "        )[y1:y2, x1:x2]\n",
        "    )\n",
        "\n",
        "    image_path = f'tempdir/{os.path.splitext(path)[0]}_{idx}.png'\n",
        "    masked_object.save(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5vnrB5S6ygAJ",
      "metadata": {
        "id": "5vnrB5S6ygAJ"
      },
      "outputs": [],
      "source": [
        "free_gpu_vars('masks', 'scores', 'sam2_predictor', 'sam2_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-GBtU5QtyrVc",
      "metadata": {
        "id": "-GBtU5QtyrVc"
      },
      "source": [
        "## Download Gemma3 model using Ollama tool."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ju0RGkY3ivYV",
      "metadata": {
        "id": "Ju0RGkY3ivYV"
      },
      "source": [
        "Run the following commands in the terminal within your colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTJBO5HilTrY",
      "metadata": {
        "id": "UTJBO5HilTrY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "curl https://ollama.ai/install.sh | sh\n",
        "ollama serve\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X_ICcwY6jazw",
      "metadata": {
        "id": "X_ICcwY6jazw"
      },
      "outputs": [],
      "source": [
        "# Pull the required open sourced LLM model.\n",
        "!ollama pull gemma3:12b-it-qat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PEyam8fLjzyq",
      "metadata": {
        "id": "PEyam8fLjzyq"
      },
      "outputs": [],
      "source": [
        "# Check if the model is downloaded.\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rnOHG6_5orqj",
      "metadata": {
        "id": "rnOHG6_5orqj"
      },
      "outputs": [],
      "source": [
        "# Prompt to analyze an image for milk packet vs others.\n",
        "prompt = \"\"\"\n",
        "Analyze the provided image of a packaging. Was this packaging used to contain milk or a milk-based product?  Answer in yes or no only.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F5HuNwRczB45",
      "metadata": {
        "id": "F5HuNwRczB45"
      },
      "source": [
        "Read an cropped images to perform inference using LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TZkeWYq_zEXs",
      "metadata": {
        "id": "TZkeWYq_zEXs"
      },
      "outputs": [],
      "source": [
        "images = glob.glob('tempdir/*.png')\n",
        "\n",
        "for path in images:\n",
        "  # Run the chat/inference API, sending the temporary masked object image as input.\n",
        "  response: ChatResponse = chat(model='gemma3:12b-it-qat', messages=[\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': prompt,\n",
        "      'images': [path]\n",
        "    },\n",
        "  ])\n",
        "  image = cv2.imread(path)\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  # Print the model's response content (the generated answer)\n",
        "  print(f\"\\n{response.message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a-YeFcFzvVV",
      "metadata": {
        "id": "4a-YeFcFzvVV"
      },
      "outputs": [],
      "source": [
        "!ollama stop gemma3:12b-it-qat"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
